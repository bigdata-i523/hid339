\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Diagnosis of Coronary Artery Disease Using Big Data Analysis}
\author{Hady Sylla}
\orcid{HID339}
  \affiliation{%
  \institution{Indiana University}
  \streetaddress{711 N.Park Ave}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47408}
}
\email{hsylla@iu.edu}



\begin{abstract}
Coronary artery disease (CAD) is the most prevalent heart disease.
Worldwide, it is among the primary causes of heart failure and mortality \cite{ali}.
Therefore, its early diagnosis is essential.
Specialists have invented different techniques for diagnosing CAD.
They conduct most of these methods using the Irvine Dataset (University of California), which have limitations of features and missing values, thus being unreliable.
The present study employs big data techniques and the latest dataset, with no missing values, and with features such as the Q wave, ST depression, functional class, T inversion, and dyspnea.
This data has been compiled from the Shaheed Rajaei Cardiovascular, Medical and Research Center, between 2011 and 2012, and involves 303 patients.
This study uses Naïve Bayes, SMO, and a proposed ensemble algorithm, using these features to conduct the analyses.
Tenfold cross-validation on the dataset has been used to design accurate algorithms with an ensemble algorithm, achieving 88.5\% accuracy.
This study extracts the relevant features and rules regarding CAD, which previous studies have not discussed.

\end{abstract}

\keywords{i523, hid339, Coronary disease, big data, Random Forest}

\maketitle

\section{Introduction}
Coronary artery disease (CAD) comes about when atherosclerotic plaques develop in the coronary arteries, giving rise to a narrowing of the coronary luminal, and sometimes occlusion, which leads to heart failure or a sudden cardiac failure.
In Western nations, CAD is among the killer diseases.
It is important for people to understand the of pathophysiology of CAD, how to control its development, to identify any effective innovation in the cardiovascular risk elements, how to diagnose and treat it in its early stages, and its reversible stages.
Experts have considered big data mining technique as the `gold standard' for diagnosing CAD, and it is commonly used.
However, big data mining is an expensive and invasive procedure that requires a high level of technological expertise and the latest technology. Thus, medical experts cannot use it to screen or treat many patients.
 For this reason, many health facilities use other noninvasive tools to diagnose CAD.
The most popular of these methods are stress echocardiography (ECHO), electrocardiogram (ECG), and scintigraphy or SPECT.
Additionally, clinical setups currently use coronary magnetic resonance angiography (CMRA) MSCT or EBCT.
The objective of this study is to show how to use big data mining technique to diagnose Coronary Artery Disease and why this is the best way to control heart attacks and deaths.
The review discusses the features in the big data mining tool, how they derive the results, and presents a discussion of the results.
Additionally, this paper provides resources for future studies and offers recommendations to improve the study of CAD.
\par Data mining is a technique for finding hidden information in a database.
Nowadays, several fields use data mining; it has different applications, such as scientific discoveries, entrepreneurship, and fraud detection.
Data mining calculations work on databases in which each information archive has several attributes.
 One of these attributes is the class label, which indicates the category of the data.
The data mining algorithms people use in problem solving are association rule mining, classification, regression analysis, and clustering.
For classification algorithms, there is required a learning phase on a labelled data set, which requires the researcher to determine the missing test record class label.
In the learning phase, a scientist constructs a classification model to predict the data label class label using the values of its features.
A specialist categorises heart disease into cardiovascular and cardiomyopathy disorders.
Coronary Artery Disease (CAD) is a major cardiomyopathy disease subgroup, which causes disability, serious illness, and even death, as the disease affects the supply of oxygen and blood to the heart muscles.
The early symptoms of heart disease comprise pain in the centre of the chest or a sense of numbness, and palpitation.
Other signs are fainting fits or dizzy spells, and dyspnea on exertion.
Due to the important nature of heart failures, it is essential to find out its causes.
Currently, the primary goal of many scientific pursuits is making an accurate diagnosis of cardiac disease.
Researchers have collected a great deal of information while examining CAD patients, and by processing the data, they can find out how the main features of cardiac disorders are related (e.g.,  the amount of cholesterol, blood pressure, etc.) and the probability of the occurrence of such a disease.
The next chapter discusses several research articles that discuss this topic and relate it to the findings of the present study.


\section{Contribution of Big Data to Coronary Artery Disease}

This section provides selected examples of the potential of big data arising from the variety, volume and value of the data people have obtained.
Additionally, it shows how big data contributes to scientific advances in cardiovascular medicine from the discovery of underlying disease mechanisms, disease taxonomy, of treatment relevant sub-types of the disease which underpin drug development, and precision medicine.

\subsection{Discovery in genetic and big data}
Lee, Noh and Ryu \cite{lee2011ire1} point out that it is challenging to provide deep mechanistic insight in large-scale big data mining resources given the limited availability of genetic information in sufficient depth.
Bespoke, recallable investigator-led studies, such as East London Genes \& Health (ELGH) and the NIHR BioResource enable the coupling of big data mining with extreme genotypes and facilitate their in-depth study using bespoke experimental protocols.
Complete gene knockouts inform people about the gene process.
With the latest survey of 3,222 British Pakistani-heritage exome sequenced persons with high parental relatedness, there have been discovered 1,111 rare-variant homozygous most likely function loss (rhLOF) genotypes to predict the disruption (knockout) of 781 genes \cite{wang2005framingham}.
\par Rajkumar \cite{rajkumar2010diagnosis} stated that linking big data to the rhLOF genotypes is not associated with a doctor or prescription consultation rate, and there are no illness-related phenotypes in 33 out of 42 individuals with rhLOF genotypes in Mendelian recessive complexity genes.
Phasing the genome sequencing of a healthy PRDM9 knockout woman, her controls, and baby, found meiotic recombination sites localized from the hotspots with PRDM9 dependency, depicting PRDM9 redundancy in human beings.
There have been genomic approaches to validating case definitions: across thousands of hospital ICD codes, there have been reproduced associations from genome-wide association studies obtained one phenotype at a time.

\subsection{Discovery in larger scale epidemiology}
Big health records data can contribute to the discovery of new associations which would be hard to generate from traditional consented cohorts without record linkage.
 This involves a resolution across a range of risk factor levels and a range of different initial presentations of cardiovascular disease.
Various associations have been  discovered in a cohort of  more than one million adults initially free from diagnosed cardiovascular disease using national structured linked electronic health records from the CALIBER resource, in which EHR phenotyping algorithms are created, validated and shared using a robust methodology \cite{wang2005framingham}.
Sideman \cite{sideman1985simulation} argued that the primary necessity for precision medicine is estimating the patient's state.
The experts mainly analyze the correlations of illness trajectories with a focus on a few complications or by the use of large-scale techniques without taking time into consideration.
Researchers have performed a discovery-driven study of illness progression patterns using data from an electronic health registry covering the entire population of Denmark.
Using the whole illness spectrum, they created 14.9 years of records data for 6,200,000 patients for 1,171 critical trajectories.  Those experts identified primary diagnoses, such as chronic obstructive pulmonary disease and gout obstructive pulmonary disease, as the cause of the progression of CAD across many of these paths. Thus, it is of added significance to diagnose CAD earlier.
Such a data-driven approach analysis is useful for preventing and predicting future complications in individual patients \cite{sideman1985simulation}.

\subsection{Discovery with deep phenotypic data}
Most cardiovascular diseases (including acute myocardial infarction) have syndromic descriptions and labels, which may span multiple underlying pathological disease processes.
One approach to discovering the mechanistically relevant disease types is to phenomap the disease \cite{sahaf2011comparing}.
For example, in heart failure with preserved ejection fraction, machine learning in 46 continuous clinical, laboratory, electrocardiographic, and echocardiographic findings have determined mutually exclusive groups related to the subsequent outcomes.
The cardiac atlas project (of healthy and diseased hearts) is an example of a large-scale collaboration on feature extraction in imaging, using data sharing in standard formats Digital Imaging and Communications in Medicine (DICOM) of the pixel and non-pixel data.
Personalization using physiological simulations, for example, for cardiac resynchronization therapy, has been proposed.
Unstructured free-text data in big data mining adds a further way to estimate disease co-occurrence and patient stratification, which people map to the biological frameworks of systems \cite{sahaf2011comparing}.


\subsection{Discovering and validating drug targets}
Regarding a discussion by \cite{rajkumar2010diagnosis}, big data-DNA resources may play an increasingly important role in drug discovery, genomic drug target validation, marker validation, and drug repurposing.
For example, \cite{rajkumar2010diagnosis} demonstrates a strategy in which human mutations that inactivate a gene encoding a drug target can mimic the action of an inhibitory drug, here ezetimibe, and thus can be used to infer potential effects of that drug.
 Ezetimibe is known to affect the marker (LDL cholesterol) but, until recently, not the disease (myocardial infarction).
Among the most significant sources of cases of MI and controls in this study was a DNA resource integrated into a health system with rich big data.  The discovery of PCSK9 as a drug target to lower cholesterol, which the EHR-DNA resources make in principle, illustrates the importance of rare variants in the identification of pathways relevant to the whole population.
Mendelian randomisation studies are essential in evaluating whether markers, such as heart rate and HDL cholesterol, are causal for the disease of interest.
Such genetic studies have questioned the role of heart rate and HDL cholesterol in the aetiology of heart attack \cite{Wijeysunderae000731}.

\subsection{Cost effectiveness of innovation}
The Institute of Medicine (U.S.), Institute of Medicine (U.S.), and Institute of Medicine (U.S.) (2011) shows that big data provide new opportunities for understanding the cost-effectiveness of existing and new interventions.
Because of the ability to assess baseline risks in unselected general populations (usually a higher risk than those reported in trials), such `real world evidence' is increasingly required by payers and regulators.
As more data sources are linked, greater granularity of the care data (e.g., 67 different types of primary care `consultation') may provide more accurate and more complete resource use data.
For example, cost-effectiveness decision models can be developed before trials, reporting an estimate of the willingness to pay and pricing of a drug according to different trial benefits (relative risk reductions) applied to patients at various strata of risk \cite{breiman2001random}.

\subsection{Public health}
Agrawal \cite{agrawal1993mining} assert that there are significant gaps in our ability to prevent the onset of and prolong life in, many of the most common cardiovascular diseases in the 21st century including atrial fibrillation, heart failure, and peripheral arterial disease.
There are also gaps in our ability to measure disease and model the impact of interventions in populations.
Clinicians diagnose more specific entities than `heart attack', `CHD' or `CVD,' yet conventional consented cohorts have lacked the statistical size or the phenotypic resolution to measure clinically relevant sub-types of disease \cite{chu2009bayesian}.
Big data can study the conditions that clinicians diagnose to provide scalable, population-based, updatable measurements of new disease burden vital for the evaluation of alternative strategies of prevention.
For example, significant data can be used to estimate the incidence and survival of the treatment-relevant sub-types of MI (ST elevation and non-ST elevation) \cite{dietterich2000ensemble}.

\section{Analysis}
To characterize the algorithms, a researcher requires a learning stage on a labelled dataset collection, which makes it possible to settle on the missing test record class \cite{ali}.
In the learning stage, a researcher builds an order model to predict the data label class label using the values of its features.
A specialist categorises heart diseases as cardiovascular and cardiomyopathy disorders \cite{ali}.
Coronary Artery Disease (CAD) is a noteworthy cardiomyopathy inconvenience subgroup which causes inability, genuine sickness, and even death, as the disease affects the supply of  oxygen and blood to the heart muscles.
The early indications of coronary illness involve torment in the focal point of the chest or a feeling of deadness, and palpitation.
Different signs include blacking out fits or spells of feeling mixed up, and dyspnea on effort.
\par  Due to the vital importance of heart failure, the primary goal of many scientific pursuits is to make an accurate diagnosis of cardiac diseases.
Analysts have gathered a lot of data while looking at CAD patients, and by preparing the information, they have uncovered how the main features highlighting cardiovascular issues are related (e.g., the measure of cholesterol, circulatory strain, and so on) and the probabilities of the occurrence of these diseases.

\subsection{Dataset}
A dataset constructed from the information collected from 303  random  visitors  (216  patients)  to  the Shaheed Rajaei Cardiovascular,  Medical  and  Research  Center was used to evaluate the effects of different demographic, clinical, and ECG features on the diagnosis of coronary artery disease.  This dataset is publicly available and does not contain any patient identifiers.  Demographics, such as age and sex, are also included in the dataset.

\subsection{Method}
I will use Random Forest machine learning for predicting the features to diagnose coronary artery disease.  Random Forests (RF) is a prominent tree-based ensemble machine learning tool that is exceptionally data adaptive \cite{breiman2001random}.  I will primarily be using the SciPy stack, especially pandas, matplotlib, seaborn, and sklearn.
I use the describe() method, which gives me the basic statistics for every column in the dataset.
This method will allow me to generate descriptive statistics that summarize the central tendency, dispersion, and shape of a dataset.

\begin{table}
\centering
\caption{Dastaset Description}
\label{my-label}
\begin{tabular}{llllllllllllllllllllll}
Age & Weight & Length & Sex & BMI   & DM        & HTN & Current Smoker & Ex-Smoker & FH & ...
& K   & Na  & WBC & Lymph & Neut & PLT & EF-TTE & Region RWMA & VHD & Cath   &        \\
0   & 53     & 90     & 175 & Male  & 29.387755 & 0   & 1              & 1         & 0  & 0   & ...
& 4.7 & 141 & 5700  & 39   & 52  & 261    & 50          & 0   & N      & Cad    \\
1   & 67     & 70     & 157 & Female & 28.398718 & 0   & 1              & 0         & 0  & 0   & ...
& 4.7 & 156 & 7700  & 38   & 55  & 165    & 40          & 4   & N      & Cad    \\
2   & 54     & 54     & 164 & Male  & 20.077335 & 0   & 0              & 1         & 0  & 0   & ...
& 4.7 & 139 & 7400  & 38   & 60  & 230    & 40          & 2   & mild   & Cad    \\
3   & 66     & 67     & 158 & Female & 26.838648 & 0   & 1              & 0         & 0  & 0   & ...
& 4.4 & 142 & 13000 & 18   & 72  & 742    & 55          & 0   & Severe & Normal \\
4   & 50     & 87     & 153 & Female & 37.165193 & 0   & 1              & 0         & 0  & 0   & ...
& 4.0 & 140 & 9200  & 55   & 39  & 274    & 50          & 0   & Severe & Normal
\end{tabular}
\end{table}


For a better visualization of the data, I will generate a histogram.
Data visualization facilitates the interpretation and breaks down big data in a manner that is easily understandable.
\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{project/images/output_2_0.png}
    \caption{Bar Plot}
    \label{Plot}
\end{figure}



\subsection{Data Preparation}
Since this dataset includes string data, I will use encoding to transform the categorical features to a format that works better with classification by taking all categorical features that have only two levels and label-encode them to get binary features.

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{project/images/Untitled3.png}
    \caption{Encoding}
    \label{Encoding}
\end{figure}

Then I will apply one hot encoding to our multiple level features, that will allow the representation of categorical data to be more expressive.

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{project/images/Encoding.png}
    \caption{Hot Encoding}
    \label{Hot Enco}
\end{figure}
    
\subsection{Measure of Variable Importance Ranking}
An essential element of RF is that it gives a quickly calculable inner measure of the significance of a variable (VIMP) that can be used to rank the factors.
This is particularly valuable for high-dimensional genomic information.  In spite of the fact that there are numerous fruitful applications using change significance, a drawback is that it is a ranking-based approach.
Ranking is significantly more troublesome than the variable determination issue, which just tries to choose a group of factors that when consolidated are prescient, without forcing a positional structure.
In any case, in view of the many-sided quality in organic frameworks, positional quality records in light of RF or RSF which consider relationship and communication impacts are still an immense change from univariate positioned quality records in light of the $t$-test's or Cox corresponding peril occurring when  utilizing one variable at any given moment.
Be that as it may, caution is required when deciphering any straight positioning since it is as a rule likely that various arrangements of feebly prescient highlights are mutually prescient.
This seems, by all accounts, to be an uncertain issue of positioning, and further investigation is required.
I will specify which dataset and variable I want to predict.  I like to keep a parameter file where I specify data sources and such.
This lets me create generic analytics code that is easy to reuse.
After I have specified what dataset I want to study, I split the training and test datasets.
Then I will scale the data, which makes most classifiers run better.

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{project/images/output_15_1.png}
    \caption{Scores Attribute}
    \label{Score}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{project/images/output_17_0.png}
    \caption{Feature importance with their standard deviations}
    \label{Featites Format}
\end{figure}

\begin{table}
\centering
\caption{Feature Importance Table}
\label{Importance}
\begin{tabular}{lll}
Importance            & Std      &          \\
Age                   & 0.086779 & 0.092552 \\
Weight                & 0.018672 & 0.032956 \\
Length                & 0.018183 & 0.027928 \\
Sex                   & 0.003414 & 0.013603 \\
BMI                   & 0.031609 & 0.042215 \\
DM                    & 0.015427 & 0.034442 \\
HTN                   & 0.038735 & 0.059296 \\
Current Smoker        & 0.005287 & 0.015965 \\
Ex-Smoker             & 0.000587 & 0.004463 \\
FH                    & 0.002472 & 0.010216 \\
Obesity               & 0.002333 & 0.008439 \\
CRF                   & 0.000534 & 0.004805 \\
CVA                   & 0.000469 & 0.004108 \\
Airway disease        & 0.000474 & 0.003385 \\
Thyroid Disease       & 0.001086 & 0.006045 \\
DLP                   & 0.000407 & 0.004046 \\
BP                    & 0.003296 & 0.013382 \\
PR                    & 0.031988 & 0.056958 \\
Edema                 & 0.014277 & 0.029568 \\
Weak Peripheral Pulse & 0.001758 & 0.007496 \\
Lung rales            & 0        & 0        \\
Systolic Murmur       & 0.001332 & 0.007931 \\
Diastolic Murmur      & 0.001853 & 0.008916 \\
Typical Chest Pain    & 0.00495  & 0.020264 \\
Dyspnea               & 0.161378 & 0.165277 \\
Function Class        & 0.006237 & 0.01781  \\
Atypical              & 0.001798 & 0.008412 \\
Nonanginal            & 0.067681 & 0.098911 \\
Exertional CP         & 0.032299 & 0.056285 \\
LowTH Ang             & 0        & 0        \\
Q Wave                & 0        & 0        \\
St Elevation          & 0.000976 & 0.005923 \\
St Depression         & 0.005336 & 0.014798
\end{tabular}
\end{table}
By checking the model's accuracy on the test set, as shown in the figure below, the accuracy score on the test by Random Forest attained 0.8852.
\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{project/images/Untitled7.png}
    \caption{Accuracy Test}
    \label{Accuracy}
\end{figure}



\section{Conclusion}
This research uses a new dataset, comprising 38 features, together with data mining methods, to get useful results about this area of research.
Afterwards, the study chooses 16 features using an element selection algorithm and some common classification algorithms, and applies the proposed ensemble algorithm to the dataset.
The research obtains the highest accuracy (88.51\%) when both the element selection algorithm and the ensemble algorithm are used.
Additionally, the study uses association rule mining methods to extract rules of high confidence from the dataset.
Coronary Artery Disease (CAD) is among the primary causes of death in the world.
Thus people must put measures in place to predict, treat, and control this disease.
The use of big data mining techniques will help achieve this and, therefore, reduce the mortality rate of people with this disease.
Additionally, not only should medical experts know the use of this technique, but the entire community, because heart failures happen at unexpected times.
\par The objectives of future studies should include adding other features, such as echo and lab echo data, to investigate the effect of these elements on CAD diagnosis and get a higher accuracy in predicting the progression of this disease.
Furthermore, future works should use more algorithms and data mining techniques to improve the findings.
Lastly, studies should extend the dataset with more sick people who can help in finding interesting results which may not be apparent for the patients in the  dataset used here.
Another aim of future research should be simplifying the use of big data mining methods to make them user friendly; this will make many medical facilities adopt the idea.
Moreover, future studies should research other tools that can improve the big data mining technique's process, thus making it less costly and require less technological know-how.

\section{Recommendations for Future Studies}
Many nations in the world have big data mining systems that research can use.  Hence, to enable the research potential of these information sources, these countries have to centralise the resources that govern the research dataset.
Some states have initiatives towards achieving this, although few tackle all the elements of this procedure.
For example, the CALIBER platform, which is in the United Kingdom, joins a repository of big data mining phenotypes with curated registered linkages combining disease registry (Myocardial Ischaemia Nation Audit Project).
Additionally, it joins primary care (Clinical Practice Research Datalink), death registry (Office of National Statistics), and hospital discharge (Hospital Episode Statistics) data for over two million adults with ten million individuals’ follow-up years.
However, this resource does not offer tools for bidirectional relations with big data mining information sources.
The United States National Human Genome Research Institute-funded consortium, the eMERGE Network, combines big data mining with a phenotype repository from multiple secondary healthcare systems, including text and imaging, joined to genotypic data for all candidates.
Lastly, the Clinical Record Interactive Search system, which is at the Maudsley NHS Foundation Trust, NIHR Mental Health Biomedical Research Centre and Dementia, and South London's Dementia Unit also help researchers.
The Clinical Record Interactive Search system allows scholars to analyze secondary data, comprising clinical notes and other texts, through simple tools that help to identify patients meet specific criteria, and help develop text-mining algorithms.
\par Regarding this, the national big data mining portals can join the strengths of these projects by including: (i) Standards-driven types of equipment that enable reviewers to create interventional and observational studies;
(ii) A countrywide catalogue of contemporary big data mining sources which is metadata standards curated;
(iii)  Big data mining phenotype algorithms with an interactive thesaurus.
By accomplishing this, the national catalogue can support metadata integration and harvest from manually curated and external sources by researchers within a reproducible and standardized framework, and also offer a guide to the data content and access.
In this way, users will be able to find sources of information that provide data both across and within disease areas.
\par The creation of big data mining datasets and algorithms should be implemented in a way that supports modification and reuse by other users, as well as the academic citation and credit that is appropriate.
Creating this form of resource enables fostering an `open source' approach to big data mining research in which researchers learn and collaborate with one another.
It will eventually produce an advance in big data mining, more than any other research group in isolation can achieve.


\begin{acks}
The authors would like to thank Professor Gregor von Laszewski and his team for assistance in providing me the tools and knowledge to complete this project.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 


\newpage
\appendix
\section{Code Reference}
All code and files for this project can be found in the githup repository:
\url{https://github.com/bigdata-i523/hid339/blob/master/project/code}


\end{document}
